{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example GDD ðŸ“¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import psycopg2\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import connect_db, get_dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>wordidx</th>\n",
       "      <th>words</th>\n",
       "      <th>poses</th>\n",
       "      <th>ners</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5705014ccf58f18a4c0d6d61</td>\n",
       "      <td>80</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>[The, authors, caution, that, projected, incre...</td>\n",
       "      <td>[DT, NNS, VBP, IN, VBN, VBN, NN, IN, NN, NN, M...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5705014ccf58f18a4c0d6d61</td>\n",
       "      <td>81</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>[-LRB-, Water, Resources, Research, ,, doi, :1...</td>\n",
       "      <td>[-LRB-, NNP, NNP, NNP, ,, FW, FW, :, NN, CD, ,...</td>\n",
       "      <td>[O, ORGANIZATION, ORGANIZATION, ORGANIZATION, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5705014ccf58f18a4c0d6d61</td>\n",
       "      <td>82</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>[American, Geophysical, Union, .]</td>\n",
       "      <td>[JJ, NNP, NNP, .]</td>\n",
       "      <td>[ORGANIZATION, ORGANIZATION, ORGANIZATION, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5705014ccf58f18a4c0d6d61</td>\n",
       "      <td>83</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>[All, Rights, Reserved, .]</td>\n",
       "      <td>[DT, NNS, VBN, .]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57ebf766cf58f1a12a8e3d34</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>[RIVER, RESEARCH, AND, APPLICATIONS, River, Re...</td>\n",
       "      <td>[NN, NN, CC, NNS, NN, NNP, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docid  sentid  \\\n",
       "0  5705014ccf58f18a4c0d6d61      80   \n",
       "1  5705014ccf58f18a4c0d6d61      81   \n",
       "2  5705014ccf58f18a4c0d6d61      82   \n",
       "3  5705014ccf58f18a4c0d6d61      83   \n",
       "4  57ebf766cf58f1a12a8e3d34       1   \n",
       "\n",
       "                                             wordidx  \\\n",
       "0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "1  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "2                                       [1, 2, 3, 4]   \n",
       "3                                       [1, 2, 3, 4]   \n",
       "4                              [1, 2, 3, 4, 5, 6, 7]   \n",
       "\n",
       "                                               words  \\\n",
       "0  [The, authors, caution, that, projected, incre...   \n",
       "1  [-LRB-, Water, Resources, Research, ,, doi, :1...   \n",
       "2                  [American, Geophysical, Union, .]   \n",
       "3                         [All, Rights, Reserved, .]   \n",
       "4  [RIVER, RESEARCH, AND, APPLICATIONS, River, Re...   \n",
       "\n",
       "                                               poses  \\\n",
       "0  [DT, NNS, VBP, IN, VBN, VBN, NN, IN, NN, NN, M...   \n",
       "1  [-LRB-, NNP, NNP, NNP, ,, FW, FW, :, NN, CD, ,...   \n",
       "2                                  [JJ, NNP, NNP, .]   \n",
       "3                                  [DT, NNS, VBN, .]   \n",
       "4                      [NN, NN, CC, NNS, NN, NNP, .]   \n",
       "\n",
       "                                                ners  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, ORGANIZATION, ORGANIZATION, ORGANIZATION, ...  \n",
       "2      [ORGANIZATION, ORGANIZATION, ORGANIZATION, O]  \n",
       "3                                       [O, O, O, O]  \n",
       "4                              [O, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get dam names\n",
    "dams = get_dams()\n",
    "\n",
    "# Database connection\n",
    "df = connect_db()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remap_sent(sent): return ' '.join(sent)\n",
    "\n",
    "\n",
    "def n_sents(idx, df):\n",
    "    ''' Returns the surrounding sentences in rel to dataframe'''\n",
    "    start = idx\n",
    "    end = idx\n",
    "    if idx > 0:\n",
    "        start = idx-1\n",
    "    if idx < len(df):\n",
    "        end = idx+1\n",
    "    return(start, end)\n",
    "\n",
    "\n",
    "def n_upper(token, sentence):\n",
    "    ''' returns uppercase tokens surrounding term '''\n",
    "    span = ''\n",
    "    idx = sentence.split().index(token)\n",
    "    while idx > 0:\n",
    "        idx = idx - 1\n",
    "        if sentence.split()[idx][0].isupper():\n",
    "            if len(span) > 1:\n",
    "                span = sentence.split()[idx] + ' ' + span\n",
    "            else:\n",
    "                span = span + sentence.split()[idx]\n",
    "        else:\n",
    "            return span\n",
    "    return span\n",
    "\n",
    "\n",
    "def sentence_elements (docid,sentid):\n",
    "    sentence_df = df[(df['docid'] == docid) & (df['sentid']== int(sentid))]\n",
    "    return sentence_df\n",
    "\n",
    "def clean_intext_references(row):\n",
    "    '''returns cleaned row (sentence elements) as df, removing in-text references'''\n",
    "    '''only addresses in-text references formated as (NAME(s) DATE)'''\n",
    "    print ((row['words'].iloc[0]))  # for testing, can be commented/removed\n",
    "\n",
    "\n",
    "    #First check to make sure sentence has '-LRB-', '-RRB-' and ners 'DATE' \n",
    "    if '-LRB-' in (row['poses'].iloc[0]) and '-RRB-' in (row['poses'].iloc[0]) \\\n",
    "    and 'DATE' in (row['ners'].iloc[0]):\n",
    "        #list of lrb and rrb indices turned into a dataframe\n",
    "        list_lrb_i = [i for i, j in enumerate(row['poses'].iloc[0]) if j == '-LRB-']\n",
    "        list_rrb_i = [i for i, j in enumerate(row['poses'].iloc[0]) if j == '-RRB-']\n",
    "        if len(list_lrb_i)==len(list_rrb_i):\n",
    "            print ('\\n' + str(len(list_lrb_i)) + \" sets of parenthesis to investigate\")  #for testing, can be commented/removed\n",
    "            #extract lists of words, poses, and ners and create df based on common list index\n",
    "            rb_df = pd.DataFrame({'lrb': list_lrb_i, 'rrb': list_rrb_i})\n",
    "            sentence_df = pd.DataFrame({'words':(row['words'].iloc[0]), 'poses':(row['poses'].iloc[0]), \\\n",
    "                                        'ners': (row['ners'].iloc[0])})\n",
    "\n",
    "            for rb in rb_df.itertuples():\n",
    "                if sentence_df['ners'][rb.rrb-1]== 'DATE' and \\\n",
    "                ([sentence_df['words'][rb.lrb:rb.rrb+1].isin(['et','al.'])] or \\\n",
    "                 [sentence_df['ners'][rb.lrb:rb.rrb+1].isin(['PERSON'])]):\n",
    "                    #drop indices of an in-text reference\n",
    "                    print ('dropping index range: ' + str(rb.lrb) + ':' + str(rb.rrb+1))  # For testing, can be commented/removed\n",
    "                    sentence_df = sentence_df.drop(sentence_df.index[(rb.lrb):(rb.rrb+1)])\n",
    "\n",
    "            #rebuild cleaned df row                \n",
    "            cleanDf = pd.DataFrame({'docid': row['docid'].iloc[0], 'sentid': row['sentid'].iloc[0], \\\n",
    "                                    'words':[sentence_df['words'].tolist()], 'poses':[sentence_df['poses'].tolist()], \\\n",
    "                                    'ners':[sentence_df['ners'].tolist()] })    \n",
    "\n",
    "            return cleanDf\n",
    "        else:\n",
    "            return row\n",
    "    else:\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove in-text references from a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Volumetric', 'change', 'estimates', 'using', '`', 'Structure-from-Motion', \"'\", 'photogrammetric', 'topography', 'analysis', '-LRB-', 'cf.', 'Westoby', 'et', 'al.', ',', '2012', '-RRB-', 'from', 'a', 'novel', 'aerial', 'imaging', 'system', 'indicate', 'that', 'a', 'total', 'of', '6.1', 'Ã—', '106', 'm3', 'of', 'sediment', 'had', 'moved', 'downstream', 'from', 'both', 'of', 'the', 'reservoir-sediment', 'de', '-', 'posits', 'as', 'of', 'spring', '2013', '-LRB-', 'A.', 'C.', 'Ritchie', ',', 'unpublished', 'data', '-RRB-', '.']\n",
      "\n",
      "2 sets of parenthesis to investigate\n",
      "dropping index range: 10:18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>ners</th>\n",
       "      <th>poses</th>\n",
       "      <th>sentid</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57ebf766cf58f1a12a8e3d34</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[JJ, NN, NNS, VBG, ``, NNP, '', JJ, NN, NN, IN...</td>\n",
       "      <td>89</td>\n",
       "      <td>[Volumetric, change, estimates, using, `, Stru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docid  \\\n",
       "0  57ebf766cf58f1a12a8e3d34   \n",
       "\n",
       "                                                ners  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                               poses  sentid  \\\n",
       "0  [JJ, NN, NNS, VBG, ``, NNP, '', JJ, NN, NN, IN...      89   \n",
       "\n",
       "                                               words  \n",
       "0  [Volumetric, change, estimates, using, `, Stru...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example that has two sets of parenthesis, one dropped due to in-text reference critera\n",
    "row = sentence_elements(docid='57ebf766cf58f1a12a8e3d34', sentid=89)\n",
    "\n",
    "clean_intext_references(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Candidate Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Dam sentences: 3150\n",
      "Candidate Stream sentences: 13512\n"
     ]
    }
   ],
   "source": [
    "cand_dam = np.zeros((len(df),), dtype=int)\n",
    "cand_stream = np.zeros((len(df),), dtype=int)\n",
    "\n",
    "for idx, i in enumerate(df['docid']):\n",
    "    doc, sentid, wordidx, words, poses, ners = df.loc[idx]\n",
    "    \n",
    "    if 'Dam' in words or 'dam' in words and 'DATE' in ners:\n",
    "        cand_dam[idx] = 1\n",
    "    if 'stream' in words or 'Stream' in words or 'River' in words or 'river' in words:\n",
    "        cand_stream[idx] = 1\n",
    "\n",
    "# add to df\n",
    "df['cand_stream'] = cand_stream\n",
    "df['cand_dam'] = cand_dam\n",
    "\n",
    "print('Candidate Dam sentences: %s' %np.unique(cand_dam, return_counts=True)[1][1])\n",
    "print('Candidate Stream sentences: %s' %np.unique(cand_stream, return_counts=True)[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling functions\n",
    "\n",
    "These are based loosely on Snorkel LFs. These are intended to filter down the candidates \n",
    "```python\n",
    "\n",
    "CandidateExtractor(Dam_Removal_Year, [ngrams, ngrams], [DateMatcher(), DictionaryMatch(d=rm)])\n",
    "\n",
    "def LF_timeframe(c):\n",
    "    ''' LF to ensure the dam removal is within a timeframe'''\n",
    "    try: \n",
    "        c = int(c.year.get_span())\n",
    "        if c > 1890 and c < 2020:\n",
    "            return 1\n",
    "        else: return 0\n",
    "    except:\n",
    "        return 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dam_extract(sent):\n",
    "    ''' Attempt to extract the name of the dam based on uppercase prior tokens '''\n",
    "    for i in sent.split():\n",
    "        if 'Dam' == i:\n",
    "            term = 'Dam'\n",
    "        if i == 'dam':\n",
    "            term = 'dam'\n",
    "    return n_upper(term, sent)\n",
    "\n",
    "\n",
    "def removal_present(series):\n",
    "    rm = ['remove', 'removal', 'breach', 'destroyed', 'destroy', 'failed', \n",
    "          'removed', 'breached', 'removing', 'post-dam', 'demolition', 'demolish',\n",
    "          'demolished', 'razing', 'razed', 'raze']\n",
    "    \n",
    "    doc, sentid, wordidx, words, poses, ners, *_ = series\n",
    "    \n",
    "    for i in rm:\n",
    "        if i in words:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def dam_nearby(sent):\n",
    "    for i in sent.split():\n",
    "        if i in dams['name'].tolist():\n",
    "            return 1 \n",
    "    return 0\n",
    "\n",
    "\n",
    "removal_present(df.iloc[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flagging Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dam Flagged: 1290 labels found\n",
      "River Flagged: 9 labels found\n",
      "Sample Dam Sentence: Dam removal is becoming an increasingly common component of river restoration -LRB- Grant , 2001 ; Pizzuto , 2002 ; Graf , 2003 -RRB- .\n",
      "Sample Stream Sentence: When signiï¬cance levels for statistical tests were provided , they are reported along with the conclusion of S or NS -LRB- signiï¬cant , non-signiï¬cant , respectively -RRB- Study Location Scale -LRB- # streams evaluated -RRB- land use Heterogeneity measure Signiï¬cance of heterogeneity effect Beisel et al. -LRB- 1998 -RRB- Harper et al. -LRB- 1997 -RRB- Minshall & Robinson -LRB- 1998 -RRB- Robson & Chester -LRB- 1999 -RRB- Buffagni et al. -LRB- 2000 -RRB- Brown -LRB- 2003 -RRB- Boyero & Bosch -LRB- 2004 -RRB- Urban et al. -LRB- 2006 -RRB- Northern France Ireland ; Czech Republic ID , U.S.A. Hobart , Tasmania North Italy NH , U.S.A. Panama CT , U.S.A. Reach and sub-reach -LRB- four streams -RRB- Forest Reach -LRB- multiple ` rivers ' -RRB- Forest Reach -LRB- 32 streams -RRB- Forest Rifï¬‚es -LRB- one stream -RRB- Unknown land use Reach -LRB- one river -RRB- ` unaltered with high water quality ' Rifï¬‚es -LRB- one stream -RRB- Forest Rifï¬‚es -LRB- one stream -RRB- Forest Multi-scale study : rifï¬‚e to watershed -LRB- 18 streams -RRB- Streams along urbanisation gradient Patch diversity and within -- patch diversity -LRB- substrate size , ï¬‚ow , depth -RRB- Habitat functional diversity -LRB- # of patch categories -RRB- Coefï¬cient of variation in substrate size Fractal dimension within patches and number of patches within a rifï¬‚e -LRB- substrate size -RRB- Identiï¬ed dominant habitats -LRB- units -RRB- and characterised each based on substrate , ï¬‚ow , depth , roughness Substrate heterogeneity -LRB- size -RRB- using diversity indices Variability in substrate types -LRB- qualitatively assessed size -RRB- and velocity in rifï¬‚e Variation in habitat , substrate and ï¬‚ow NS -- No difference in species richness across patches that differed in substrate or patch ` richness ' -LRB- i.e. heterogeneity -RRB- but abstract makes conï¬‚icting statements S -- Within-patch -LRB- bryophyte patches -RRB- complexity was associated with more species NS -- Could not distinguish between main channel -LRB- with lower habitat diversity -RRB- and ï¬‚oodplain regions with high habitat diversity ; attribute it to water quality differences NS -- Compared substrate Coefï¬cient of Variablity to species richness among the three groups of streams S -- P < 0.05 ; More species on boulder-cobble rifï¬‚es than in bedrock rifï¬‚es NS -- Paper focused on showing that species habitat type -LRB- unit -RRB- is not a surrogate for biota but authors also provide data on roughness , means and SD for ï¬‚ow , depth , grain size .\n"
     ]
    }
   ],
   "source": [
    "flagged_dam = np.zeros((len(df),), dtype=int)\n",
    "flagged_stream = np.zeros((len(df),), dtype=int)\n",
    "\n",
    "# Get candidate dams \n",
    "for idx, i in df[df['cand_dam'] == 1].iterrows():\n",
    "    start, end = n_sents(idx, df)\n",
    "    \n",
    "    for sent in [start, idx, end]:\n",
    "        if removal_present(df.iloc[sent]) == 1:\n",
    "            flagged_dam[idx] = 1\n",
    "\n",
    "# Get candidate rivers\n",
    "for idx, i in df[df['cand_stream'] == 1].iterrows():\n",
    "    start, end = n_sents(idx, df)\n",
    "    \n",
    "    for sent in [start, idx, end]:\n",
    "        if dam_nearby(remap_sent(df.iloc[sent]['words'])) == 1:\n",
    "            flagged_stream[idx] = 1\n",
    "\n",
    "\n",
    "# Add to dataframe\n",
    "df['flagged_dam'] = flagged_dam\n",
    "df['flagged_stream'] = flagged_stream\n",
    "\n",
    "# Display totals\n",
    "print('Dam Flagged: %s labels found' %df[df['flagged_dam'] == 1].shape[0])\n",
    "print('River Flagged: %s labels found' %df[df['flagged_stream'] == 1].shape[0])\n",
    "\n",
    "sample_flagged_dam = df['words'][df['flagged_dam']==1].iloc[5]\n",
    "sample_flagged_stream = df['words'][df['flagged_stream']==1].iloc[5]\n",
    "print('Sample Dam Sentence: %s' %remap_sent(sample_flagged_dam))\n",
    "print('Sample Stream Sentence: %s' % remap_sent(sample_flagged_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Removal Year Extraction\n",
    "\n",
    "REDO THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cand in [40:90]:\n",
    "    doc, sentid, wordidx, words, poses, ners, *_ = df.loc[cand]\n",
    "    if 'dam' in words or 'Dam' in words:\n",
    "        dam = dam_extract(remap_sent(df['words'].iloc[cand]))\n",
    "        try:\n",
    "            if dam != '':\n",
    "                dates = []\n",
    "                for idx, i in enumerate(ners): \n",
    "                    if i == 'DATE':\n",
    "                        dates.append(words[idx])\n",
    "                if len(dates) > 0: print(dam, dates, remap_sent(df['words'].iloc[cand]), doc)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
